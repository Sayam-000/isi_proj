{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RBciFg1XrviE"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from skimage import io, transform\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torchvision.models as tmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHqDZqBfr_jR"
   },
   "source": [
    "**Multi Label Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dT5kQbzQsHk4"
   },
   "source": [
    "Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u9H_OQtVsJ-Y"
   },
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,self.annotations.iloc[idx, 1])\n",
    "        image = io.imread(img_name)\n",
    "        labels = self.annotations.iloc[idx, 2]\n",
    "\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            \n",
    "#         print(\"sample\",sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bhwUYY-JE0Hh"
   },
   "outputs": [],
   "source": [
    "classes = {'T-Shirt','Trouser','PullOver','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'}\n",
    "\n",
    "batch_size=4\n",
    "label_count=len(classes)\n",
    "\n",
    "\n",
    "\n",
    "trainset = MultiLabelDataset(csv_file = '/home/dnn4/pythonCodeArea/sayam/train/labels.csv',root_dir = '/home/dnn4/pythonCodeArea/sayam/train/',transform = transforms.Compose([transforms.ToTensor()]))\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=10)\n",
    "testset = MultiLabelDataset(csv_file = '/home/dnn4/pythonCodeArea/sayam/test/labels.csv', root_dir = '/home/dnn4/pythonCodeArea/sayam/test/',transform = transforms.Compose([transforms.ToTensor()]))\n",
    "testloader = DataLoader(trainset, batch_size=batch_size,shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]',\n",
       " '[1, 0, 1, 1, 1, 0, 0, 0, 0, 0]',\n",
       " '[0, 0, 1, 0, 1, 0, 0, 1, 0, 1]',\n",
       " '[0, 0, 0, 0, 0, 0, 1, 1, 1, 0]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=next(iter(testloader))\n",
    "img['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffpergPL5TXR"
   },
   "source": [
    "Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XDtyzJHE5Wsc"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        res_state=tmodels.resnet18(pretrained=True)\n",
    "        self.cnn=nn.Sequential(*list(res_state.children())[:-1])\n",
    "        self.f3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.cnn(x)\n",
    "        x=torch.squeeze(x)\n",
    "#         print(\"x.shape\",x.shape)\n",
    "        x = self.f3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=torch.rand(4,1,28,128)\n",
    "\n",
    "# x=x.repeat(1, 3, 1,1)\n",
    "# x=x.cuda()\n",
    "# y=net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMI3cCNu5a9E"
   },
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eC9OZhMO6iuU"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimiser = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1jLG0KL7kHI"
   },
   "source": [
    "Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tHFvY27E7sEW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss:  0.0 accuracy: 25.0\n",
      "epoch: 2 loss:  0.0 accuracy: 25.0\n",
      "epoch: 3 loss:  0.0 accuracy: 25.0\n",
      "epoch: 4 loss:  0.0 accuracy: 25.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # Iterations over the dataset\n",
    "    loss_at_iteration = 0.0\n",
    "    total=0.0\n",
    "    correct=0\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels= data['image'], data['labels']\n",
    "        inputs=inputs.repeat(1, 3, 1,1)\n",
    "#         print('labels',labels)\n",
    "#         for t in range(len(labels)):\n",
    "#             labels[t]=list(labels[t])\n",
    "#         print('labels',labels)\n",
    "            \n",
    "        m=torch.zeros([batch_size,label_count])\n",
    "        for i in range(batch_size):\n",
    "            for j in range(label_count):\n",
    "                if labels[i][j]!='[' and labels[i][j]!=']' and labels[i][j]!=',' and labels[i][j]!=' ':\n",
    "                    m[i][j]=int(labels[i][j]) \n",
    "                    \n",
    "        inputs = inputs.cuda(), \n",
    "#         labels = torch.Tensor(labels)\n",
    "        m = m.cuda()\n",
    "\n",
    "        # set the parameter gradients to zero\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward backward and optimise\n",
    "        outputs = net(inputs[0])\n",
    "        \n",
    "#         print(type(outputs),type(labels))\n",
    "        loss = criterion(outputs, m)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        \n",
    "        predicted=torch.topk(outputs, 4)\n",
    "        predicted_indices = predicted[1]        \n",
    "\n",
    "        total += m.shape[0]*4\n",
    "        \n",
    "        multi_hot_GT=torch.zeros([batch_size,label_count]).cuda()\n",
    "        for i in range(batch_size):\n",
    "            for p in range(len(predicted_indices[i])):\n",
    "                multi_hot_GT[i][predicted_indices[i]]=1        \n",
    "        \n",
    "#         print(\"(multi_hot_GT == m1).sum().item()\",(multi_hot_GT == m1).sum().item())\n",
    "        for i in range(batch_size):\n",
    "            for j in range(label_count):\n",
    "                if m[i][j]==1 and multi_hot_GT[i][j]==1:\n",
    "                    correct+=1 \n",
    "        \n",
    "        \n",
    "        #if i % 100 == 99:    # print every 2000th mini-batche\n",
    "#     print(f'{epoch + 1} epoch loss: {loss_at_iteration.3f} accuracy: {100 * correct // total}')\n",
    "    print('epoch:', epoch + 1,'loss: ',loss_at_iteration, 'accuracy:', 100 * correct // total)\n",
    "\n",
    "    loss_at_iteration = 0.0\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wrMcBuVI8kT0"
   },
   "outputs": [],
   "source": [
    "#To Save The Network\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPx41Dim8lXt"
   },
   "source": [
    "Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rHSxwzo78pT2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function to show image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "images, labels = data_iter.next()['image'] , data_iter.next()['labels']\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "#print('Ground Truth: ', ' '.join(f'{classes[labels[j][0]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CEuTBOl984Z6"
   },
   "outputs": [],
   "source": [
    "#Reload the saved Model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net = net.cuda()\n",
    "images, m = images.cuda(), m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xurWJPSj88LA"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[4, 1, 28, 112] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         images, m1 \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), m1\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         print(\"images.shape\",images.shape)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# outputs by running images through the network\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# the class with the highest energy is what we choose as prediction\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \n\u001b[1;32m     25\u001b[0m         \n\u001b[1;32m     26\u001b[0m         \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         _, predicted = torch.sort(outputs.data, 1)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         predicted\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtopk(outputs, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqueeze(x)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#         print(\"x.shape\",x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:399\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:395\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    393\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    394\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[4, 1, 28, 112] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# outputs = net(images[0])\n",
    "# _,predicted = torch.sort(outputs,1)\n",
    "# predicted = predicted[-4:]\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data['image'],data['labels']\n",
    "        m1=torch.zeros([batch_size,label_count])\n",
    "        for i in range(batch_size):\n",
    "            for j in range(label_count):\n",
    "                if labels[i][j]!='[' and labels[i][j]!=']' and labels[i][j]!=',' and labels[i][j]!=' ':\n",
    "                    m1[i][j]=int(labels[i][j]) \n",
    "                    \n",
    "        images, m1 = images.cuda(), m1.cuda()\n",
    "        \n",
    "        \n",
    "#         print(\"images.shape\",images.shape)\n",
    "        # outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        \n",
    "        \n",
    "        \n",
    "#         _, predicted = torch.sort(outputs.data, 1)\n",
    "        predicted=torch.topk(outputs, 4)\n",
    "        predicted_indices = predicted[1]\n",
    "        \n",
    "        multi_hot_GT=torch.zeros([batch_size,label_count]).cuda()\n",
    "        for i in range(batch_size):\n",
    "            for p in range(len(predicted_indices[i])):\n",
    "                multi_hot_GT[i][predicted_indices[i]]=1\n",
    "\n",
    "#         print(\"m1\",m1)#,predicted)\n",
    "#         print(\"multi_hot_GT\",multi_hot_GT)\n",
    "        \n",
    "        total += m1.shape[0]*4\n",
    "#         print(\"(multi_hot_GT == m1).sum().item()\",(multi_hot_GT == m1).sum().item())\n",
    "        for i in range(batch_size):\n",
    "            for j in range(label_count):\n",
    "                if m1[i][j]==1 and multi_hot_GT[i][j]==1:\n",
    "                    correct+=1\n",
    "        \n",
    "        \n",
    "#         correct += (multi_hot_GT == m1).sum().item()\n",
    "        \n",
    "        \n",
    "\n",
    "print(f'Accuracy on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_func(multihot_batch):\n",
    "    print(multihot_batch.shape)\n",
    "#     multihot_batch = torch.tensor([[0,1,0,1], [0,0,0,1], [0,0,1,1]])\n",
    "    #multihot_batch = torch.tensor([[0,1,0,1], [0,0,0,1], [0,0,1,1], [0,0,0,1]])\n",
    "    vnon = (multihot_batch == torch.tensor(1)).nonzero(as_tuple=False).cuda()\n",
    "    v0 = vnon[:,0].cuda()\n",
    "    v1 = vnon[:,1].cuda()\n",
    "\n",
    "    # 0-based index -> 1-based index\n",
    "    split_ind = ((torch.roll(v0, -1, 0) - v0) == 1).nonzero(as_tuple=False)[:,0] + 1\n",
    "    split_ind = split_ind.cuda()\n",
    "    print(split_ind.shape)\n",
    "    # the first index is excatly the split size of the first split\n",
    "    # the other splits(apart from the last one) can be obtained by this\n",
    "    split_size = torch.cat([split_ind[0].view(1),(torch.roll(split_ind, -1, 0) - split_ind)[:-1]])\n",
    "    # add the final split size\n",
    "    split_size=split_size.cuda()\n",
    "    \n",
    "    final_size = torch.tensor([torch.numel(v1) - torch.sum(split_size)])\n",
    "    \n",
    "    final_size=final_size.cuda()\n",
    "    split_size = torch.cat([split_size, final_size])\n",
    "    \n",
    "    return torch.split(v1, split_size.tolist())\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihot_batch = torch.tensor([[0,1,0,1], [0,0,0,1], [0,0,1,1]])\n",
    "multihot_batch.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
